<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="UTF-8">
<meta name="description" content="Explore the critical yet poorly studied relationship between vision backbones and optimizers, revealing the phenomenon of backbone-optimizer coupling bias (BOCB) for the first time.">
<meta name="keywords" content="Provide the first backbone-optimizer benchmark that encompasses a wide range of mainstream vision backbones.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>BOCB</title>
<link rel="icon" href="images/logo1.jpg" type="image/jpg">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
<link rel="stylesheet" href="./static/css/index.css">
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
<link rel="stylesheet" href="./static/css/leaderboard.css">
<script src="./static/js/leaderboard_testmini.js"></script>

<style>
body {
    font-family: 'Google Sans', 'Noto Sans', sans-serif;
    background: linear-gradient(to right, #f7f7f7, #e9ecef);
    color: #333;
}

h1, h2, h3 {
    font-family: 'Castoro', serif;
    color: #333;
    text-align: center;
}

h1 {
    font-size: 3em;
}

h2 {
    font-size: 2.5em;
}

h3 {
    font-size: 2em;
}

.section {
    padding: 40px 20px;
    transition: background-color 0.3s ease;
}

.section:hover {
    background-color: #f5f5f5;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
}

.has-text-centered {
    text-align: center;
}

.content {
    margin-top: 20px;
    font-size: 1.1em;
    line-height: 1.6;
}

.centered-text {
    text-align: center;
}

.publication-links .button {
    margin: 0.5rem;
    border-radius: 30px;
    transition: background-color 0.3s ease, transform 0.3s ease;
}

.button:hover {
    background-color: #00d1b2;
    border-color: transparent;
    transform: scale(1.05);
}

.table-container {
    overflow-x: auto;
    width: 100%;
    min-height: 200px;
}

table {
    width: 100%;
    border-collapse: collapse;
}

th, td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: center;
    transition: background-color 0.3s ease;
}

th {
    background-color: #f2f2f2;
    position: sticky;
    top: 0;
    z-index: 20;
}

.chig, .chhg, .clow, .cllw {
    font-weight: bold;
}

.chhg { 
    color: #001a33; 
    background-color: #a3d1ff; 
    font-weight: bold;
}

.chig { 
    color: #004080; 
    background-color: #b3d9ff;
    font-weight: normal; 
}

.clow { 
    color: #555555;
    background-color: #aaaaaa;
    font-weight: normal;
}

.cllw { 
    color: #888888;
    background-color: #cccccc;
}

.chig:hover { background-color: #80bfff; }
.chhg:hover { background-color: #99ccff; } 
.clow:hover { background-color: #999999; }
.cllw:hover { background-color: #bbbbbb; }

.expandable-card .card-text-container {
max-height: 200px;
overflow-y: hidden;
position: relative;
}

.expandable-card.expanded .card-text-container {
max-height: none;
}

.expand-btn {
position: relative;
display: none;
background-color: rgba(255, 255, 255, 0.8);
color: #510c75;
border-color: transparent;
transition: background-color 0.3s ease;
}

.expand-btn:hover {
background-color: rgba(200, 200, 200, 0.8);
text-decoration: none;
border-color: transparent;
color: #510c75;
}

.expand-btn:focus {
outline: none;
text-decoration: none;
}

.expandable-card:not(.expanded) .card-text-container::after {
content: "";
position: absolute;
bottom: 0;
left: 0;
width: 100%;
height: 90px;
background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
}

.expandable-card:not(.expanded) .expand-btn {
margin-top: -40px;
}

.card-body {
padding-bottom: 5px;
}

.vertical-flex-layout {
justify-content: center;
align-items: center;
height: 100%;
display: flex;
flex-direction: column;
gap: 5px;
}

.figure-img {
max-width: 100%;
height: auto;
transition: transform 0.3s ease;
}

.figure-img:hover {
transform: scale(1.05);
}

.adjustable-font-size {
font-size: calc(0.5rem + 2vw);
}

.chat-history {
flex-grow: 1;
overflow-y: auto;
padding: 5px;
border-bottom: 1px solid #ccc;
margin-bottom: 10px;
}

#gradio pre {
background-color: transparent;
}

.hero {
background: #fff;
padding: 3rem 1.5rem;
box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.publication-title {
color: #333;
font-weight: 700;
}
.publication-authors a {
color: #f68946;
font-weight: normal;
}

.publication-links .button {
margin: 0.5rem;
}

.subtitle span {
color: #ff3860;
}

.btn-group .btn {
font-size: 1.2em;
border-radius: 50%;
transition: background-color 0.3s ease;
}

.btn-group .btn:hover {
background-color: #00d1b2;
}

.hero {
background-color: #f5f5f5;
padding: 2rem 1rem;
border-bottom: 1px solid #ddd;
}

.hero-body {
padding: 2rem 1.5rem;
}

.container {
max-width: 1200px;
margin: 0 auto;
}

.columns {
display: flex;
justify-content: center;
}

.column {
padding: 1rem;
text-align: center;
}

.column img {
max-width: 90%;
height: auto;
border: 5px solid #fff;
box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
transition: transform 0.3s ease-in-out;
}

.column img:hover {
transform: scale(1.05);
}

.maintainers {
  display: flex;
  justify-content: center;
  gap: 20px; /* Ë∞ÉÊï¥Èó¥Ë∑ù */
}

.maintainer {
  text-align: center;
}

.maintainer img {
  width: 100px;
  height: 100px;
  border-radius: 50%;
  object-fit: cover;
}

.maintainer span {
  display: block;
  margin-top: 5px;
}
@media (max-width: 768px) {
.column img {
  max-width: 100%;
}
}
</style>
</head>

<body>
<section class="hero">
<div class="hero-body">
<div class="container">
<div class="columns is-centered">
<div class="column has-text-centered">
  <h1 class="title publication-title" style="font-size: 3em; text-align: center;">
    <img src="images/logo1.jpg" alt="Logo" class="static-image" style="width: 2.11em; height: 3em;">
  </h1>
  <h3 class="title publication-title">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</h3>
  <div class="is-size-5 publication-authors">
    <span class="author-block"><a href="https://lupin1998.github.io/" style="color:#f68946;"><strong>Siyuan Li<sup>1,2*</sup></strong></a>,</span>
    <span class="author-block"><a href="https://tianshijing.github.io/" style="color:#f68946;"><strong>Juanxi Tian<sup>1,*</sup></strong></a>,</span>
    <span class="author-block"><a href="https://jacky1128.github.io/" style="color:#f68946;"><strong>Zedong Wang<sup>1,*</sup></strong></a>,</span>
    <span class="author-block"><a href="" style="color:#f68946;">Luyuan Zhang<sup>1</sup></a>,</span>
    <span class="author-block"><a href="https://pone7.github.io/" style="color:#f68946;">Zicheng Liu<sup>1,2</sup></a>,</span>
    <span class="author-block"><a href="" style="color:#f68946;">Weiyang Jin<sup>1</sup></a>,</span>
    <span class="author-block"><a href="" style="color:#008AD7;">Yang Liu<sup>3</sup></a>,</span>
    <span class="author-block"><a href="" style="color:#008AD7;">Baigui Sun<sup>3</sup></a>,</span>
    <span class="author-block"><a href="" style="color:#f68946;">Stan Z. Li<sup>1,‚Ä†</sup></a>,</span>
  </div>
  <div class="is-size-5 publication-authors">
    <span class="author-block"><b style="color:#f68946;">&#x25B6</b> <sup>1</sup>AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China</span>
    <span class="author-block"><b style="color:#00d707;">&#x25B6</b> <sup>2</sup>Zhejiang University, Hangzhou, China</span>
    <span class="author-block"><b style="color:#008AD7;">&#x25B6</b> <sup>2</sup>DAMO Academy, Hangzhou, China</span>
  </div>
  <div class="is-size-6 publication-authors">
    <span class="author-block"><sup>*</sup>Equal Contribution</span>
    <span class="author-block"><sup>‚Ä†</sup>Corresponding author</span>
  </div>
  <div class="column has-text-centered">
    <div class="publication-links">
      <span class="link-block">
        <a href="https://arxiv.org/abs/2406.06007" target="_blank" class="external-link button is-normal is-rounded is-dark">
          <span class="icon"><i class="ai ai-arxiv"></i></span>
          <span>arXiv</span>
        </a>
      </span>
      <span class="link-block">
        <a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" class="external-link button is-normal is-rounded is-dark">
          <span class="icon"><i class="fab fa-github"></i></span>
          <span>Code</span>
        </a>
      </span>
      <span class="link-block">
        <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
          <span class="icon"><i class="far fa-images"></i></span>
          <span>Benchmark</span>
        </a>
      </span>
    </div>
  </div>
</div>
</div>
</div>
</div>
</section>
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="images/BOCB_true.png" alt="BOCB Image" style="max-width: 90%; height: auto;">
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="background-color:#efeff081">
    <div class="container">
    <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
    <p>
    This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed <strong><em>backbone-optimizer coupling bias (BOCB)</em></strong>. We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The <a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" style="color: blue; text-decoration: underline;">source code</a> and models are publicly available. </p>
    </div>
    </div>
    </div>
    </div>
</section>

<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <h2 class="title is-3">Introduction</h2>
      <h4 class="subtitle has-text-centered">
        üåü<span>[NEW!]</span> We explore the crucial yet poorly studied backbone-optimizer interplay in visual representation learning, revealing the phenomenon of <strong><em>backbone-optimizer coupling bias (BOCB)</em></strong>.
        <br><br>
        üåü<span>[NEW!]</span> We provide the backbone-optimizer benchmark that encompasses 20 popular vision backbones, from classical CNNs to recent transformer-based architectures, and evaluate their performance against 20 mainstream optimizers on CIFAR-100, ImageNet-1K, and COCO, unveiling the practical limitations introduced by BOCB in both pre-training and transfer learning scenarios.
        <br><br>
        üåü<span>[NEW!]</span> From the BOCB perspective, we summarize optimizers recommendations and insights on more robust vision backbone design. The benchmark results also serve as takeaways for user-friendly deployment. We open-source the code and models for further explorations in the community.
        <br><br>
        <strong style="font-size: 1.2em;">ü§îAs well as discussions on many extended topics, such as whether the long-standing debate between CNNs and Transformers has been settled (especially in an era where model scaling up/down is becoming increasingly important), and whether there are better optimizers that can replace AdamW after years of development, and so on. These topics may find clearer answers following the BOCB.</strong>
        <br><br>
        <span style="font-size: 1.5em; font-weight: bold; color: black;">&#8595;</span>
        <br><br>
        <strong style="font-size: 1.2em;">Q1:</strong> Does any identifiable dependency exist between existing vision backbones and widely-used optimizers? <br><br>
        <strong style="font-size: 1.2em;">Q2:</strong> If such backbone-optimizer dependencies exist, (how) do they affect the training dynamics and performance of vision models? <br><br>
        <strong style="font-size: 1.2em;">Q3:</strong> Can we identify direct connections between these inter-dependencies and specific designs of vision backbone architectures and optimizers? <br><br>
      </h4>
    </div>
  </div>
</section>

<section class="section" style="background-color:#efeff081">
    <div class="container">
    <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
    <h2 class="title is-3">New team & New series work!</h2>
    <img src="images/logo1.jpg" alt="Logo" class="static-image" style="width: 2.11em; height: 3em;">
    <div class="content has-text-justified">
    <p>A new team, Black-Box Optimization & Coupling Bias (BOCB), has been established by Juanxi Tian, Siyuan Li, and Zedong Wang. We believe that our first work, 'Backbone-Optimizer Coupling Bias' marks a promising start for our team. Moving forward, we will focus on a broader range of scenarios in Deep Learning, exploring more intriguing and even counterintuitive phenomena to better understand the Coupling Bias in complex DL systems. We aim to make significant optimizations and improvements. Stay tuned for more exciting developments.</p>
    </div>
    </div>
    </div>
    </div>
</section>

<section class="section" style="background-color:#f7f7f7">
<div class="container">
<div class="columns is-centered has-text-centered">
<div class="column is-six-fifths">
<h2 class="title is-3">Visual Insights</h2>
<div class="slider">
<div class="slide">
<h3>Backbone Roadmap.</h3>
<img src="images/Macro_design.png" alt="Backbone Roadmap">
</div>
<div class="slide">
<h3>General Algorithm of Optimizer for DNNs.</h3>
<img src="images/Opt.png" alt="General Algorithm of Optimizer for DNNs">
</div>
<div class="slide">
<h3>Four categories optimizer.</h3>
<img src="images/Optimizer.svg" alt="Four categories optimizer">
</div>
<div class="slide">
<h3>Violinplot of the performance stability for different backbones.</h3>
<img src="images/violinplot_backbone_hyper.png" alt="Hyper-parameters consistency">
</div>
<div class="slide">
<h3>Boxplot visualization of hyper-parameter robustness.</h3>
<img src="images/violinplot_backbone_hyper.png" alt="Hyper-parameters consistency">
</div>
<div class="slide">
<h3>Boxplot of optimizers generality.</h3>
<img src="images/violinplot_opt_hyper.png" alt="Optimizer generality">
</div>
<div class="slide">
<h3>Model parameter patterns.</h3>
<img src="images/model_pattern.jpg" alt="Model parameter patterns">
</div>
</div>
<button class="slider-button prev" onclick="moveSlide(-1)">&#10094;</button>
<button class="slider-button next" onclick="moveSlide(1)">&#10095;</button>
</div>
</div>
</div>
</section>
<style>
.slider {
position: relative;
overflow: hidden;
}
.slide {
display: none;
text-align: center;
}
.slide img {
width: 100%;
height: auto;
}
.slider-button {
position: absolute;
top: 50%;
transform: translateY(-50%);
background-color: rgba(0, 0, 0, 0.5);
color: white;
border: none;
padding: 10px;
cursor: pointer;
}
.slider-button.prev {
left: 10px;
}
.slider-button.next {
right: 10px;
}
</style>

<script>
let slideIndex = 0;
showSlide(slideIndex);

function moveSlide(n) {
showSlide(slideIndex += n);
}

function showSlide(n) {
const slides = document.querySelectorAll('.slide');
if (n >= slides.length) { slideIndex = 0 }
if (n < 0) { slideIndex = slides.length - 1 }
slides.forEach(slide => slide.style.display = 'none');
slides[slideIndex].style.display = 'block';
}
</script>
<section class="section">
  <div class="container">
      <div class="columns is-centered">
          <div class="column is-full has-text-centered content">
              <h2 class="title is-3" id="leaderboard">CIFAR100 Benchmark</h2>
              <div class="content">
                  <p class="mt-3">Top-1 accuracy (\%) of representative vision backbones with 20 popular optimizers on CIFAR-100. The torch-style training settings are used for AlexNet, VGG-13, R-50 (ResNet-50), DN-121 (DenseNet-121), MobV2 (MobileNet.V2), and RVGG-A1 (RepVGG-A1), while other backbones adopt modern recipes, including Eff-B0 (EfficientNet-B0), ViT variants, ConvNeXt variants (CNX-T and CNXV2-T), Moga-S (MogaNet-S), URLK-T (UniRepLKNet-T), and TNX-T (TransNeXt-T). We list MetaFormer S12 variants apart from other modern DNNs as IF-12, PFV2-12, CF-12, AF-12, and CAF-12.</p>
                  <p>The <span style="background-color: lightblue;">blue</span> and <span style="background-color: #aaaaaa;">gray</span> features denote the top-4 and trivial results, while others are inliers.</p>
                  <p>Two bottom lines report mean, std, and range on statistics that removed the worst result for all models.</p>
                  <p class="mt-3"><strong>You can swipe left and right to see the full table.</strong></p>  
                  <div class="table-container">
                  <table class="js-sort-table" id="results">
                      <thead>
                          <tr>
                              <th>Backbone</th>
                              <th>AlexNet</th>
                              <th>VGG-13</th>
                              <th>R-50</th>
                              <th>DN-121</th>
                              <th>MobV2</th>
                              <th>Eff-B0</th>
                              <th>RVGG-A1</th>
                              <th>DeiT-S</th>
                              <th>MLP-S</th>
                              <th>Swin-T</th>
                              <th>CNX-T</th>
                              <th>CNXV2-T</th>
                              <th>Moga-S</th>
                              <th>URLK-T</th>
                              <th>TNX-T</th>
                              <th>IF-12</th>
                              <th>PFV2-12</th>
                              <th>CF-12</th>
                              <th>AF-12</th>
                              <th>CAF-12</th>
                          </tr>
                      </thead>
                      <tbody>
                          <tr>
                              <td>SGD-M</td>
                              <td class="chig">66.76</td>
                              <td class="chig">77.08</td>
                              <td>78.76</td>
                              <td>78.01</td>
                              <td>77.16</td>
                              <td class="chig">79.41</td>
                              <td>75.85</td>
                              <td class="cllw">63.20</td>
                              <td>78.95</td>
                              <td class="cllw">60.09</td>
                              <td>82.25</td>
                              <td class="clow">75.93</td>
                              <td>82.75</td>
                              <td>86.21</td>
                              <td>77.40</td>
                              <td>77.70</td>
                              <td>83.46</td>
                              <td>83.02</td>
                              <td>81.21</td>
                          </tr>
                        <tr>
                          <td>SGDP</td>
                          <td>66.54</td>
                          <td class="chhg">77.56</td>
                          <td class="chig">79.25</td>
                          <td class="chig">78.93</td>
                          <td>77.32</td>
                          <td class="chig">79.55</td>
                          <td>75.26</td>
                          <td class="clow">63.53</td>
                          <td class="clow">69.24</td>
                          <td>80.56</td>
                          <td class="clow">61.25</td>
                          <td>82.43</td>
                          <td class="clow">80.86</td>
                          <td>82.18</td>
                          <td>86.12</td>
                          <td>77.55</td>
                          <td>77.53</td>
                          <td>83.54</td>
                          <td>82.88</td>
                          <td>81.56</td>
                      </tr>
                      <tr>
                          <td>LION</td>
                          <td>62.11</td>
                          <td>73.87</td>
                          <td>75.28</td>
                          <td>75.42</td>
                          <td>74.62</td>
                          <td>76.97</td>
                          <td>73.55</td>
                          <td class="chig">74.57</td>
                          <td class="chig">74.19</td>
                          <td class="chig">81.84</td>
                          <td>82.29</td>
                          <td>82.53</td>
                          <td>85.03</td>
                          <td>83.43</td>
                          <td>86.96</td>
                          <td>78.65</td>
                          <td>79.66</td>
                          <td>84.62</td>
                          <td>82.41</td>
                          <td class="clow">79.59</td>
                      </tr>
                      <tr>
                          <td>Adam</td>
                          <td>65.29</td>
                          <td>73.41</td>
                          <td>74.55</td>
                          <td>76.78</td>
                          <td>74.56</td>
                          <td>76.48</td>
                          <td>75.06</td>
                          <td>71.04</td>
                          <td>72.84</td>
                          <td>80.71</td>
                          <td>82.03</td>
                          <td>82.66</td>
                          <td>84.92</td>
                          <td>84.73</td>
                          <td>86.23</td>
                          <td>78.39</td>
                          <td>79.18</td>
                          <td>84.81</td>
                          <td>81.54</td>
                          <td>82.18</td>
                      </tr>
                      <tr>
                          <td>Adamax</td>
                          <td class="chhg">67.30</td>
                          <td>73.80</td>
                          <td>75.21</td>
                          <td class="clow">73.52</td>
                          <td>74.60</td>
                          <td>78.37</td>
                          <td>74.33</td>
                          <td>73.31</td>
                          <td>73.07</td>
                          <td>81.28</td>
                          <td>80.25</td>
                          <td>81.90</td>
                          <td>84.51</td>
                          <td>83.81</td>
                          <td>86.34</td>
                          <td>78.02</td>
                          <td>79.55</td>
                          <td>84.31</td>
                          <td>81.83</td>
                          <td>82.50</td>
                      </tr>
                      <tr>
                          <td>NAdam</td>
                          <td class="clow">60.49</td>
                          <td>73.96</td>
                          <td>74.82</td>
                          <td>76.10</td>
                          <td>75.08</td>
                          <td>77.06</td>
                          <td>74.86</td>
                          <td>72.75</td>
                          <td>73.77</td>
                          <td>81.80</td>
                          <td>82.26</td>
                          <td>82.72</td>
                          <td>85.23</td>
                          <td>82.07</td>
                          <td>86.44</td>
                          <td>78.37</td>
                          <td class="chig">80.32</td>
                          <td>84.81</td>
                          <td>81.82</td>
                          <td>82.83</td>
                      </tr>
                      <tr>
                          <td>AdamW</td>
                          <td>62.71</td>
                          <td>73.90</td>
                          <td>75.56</td>
                          <td>78.14</td>
                          <td>76.88</td>
                          <td>78.77</td>
                          <td>75.35</td>
                          <td>72.15</td>
                          <td>73.59</td>
                          <td>81.30</td>
                          <td>83.52</td>
                          <td>83.59</td>
                          <td class="chig">86.19</td>
                          <td class="chig">86.30</td>
                          <td class="chig">87.51</td>
                          <td class="chig">79.39</td>
                          <td class="chig">80.55</td>
                          <td class="chig">85.46</td>
                          <td>82.24</td>
                          <td class="chig">83.60</td>
                      </tr>
                      <tr>
                          <td>LAMB</td>
                          <td class="chig">66.90</td>
                          <td>75.55</td>
                          <td>77.19</td>
                          <td>78.81</td>
                          <td class="chig">77.59</td>
                          <td>78.77</td>
                          <td class="chig">77.04</td>
                          <td class="chig">75.39</td>
                          <td class="chhg">74.98</td>
                          <td class="chhg">83.47</td>
                          <td class="chig">84.13</td>
                          <td class="chhg">84.93</td>
                          <td class="chig">86.04</td>
                          <td>84.99</td>
                          <td class="chig">87.37</td>
                          <td class="chig">80.21</td>
                          <td>80.01</td>
                          <td class="chig">85.40</td>
                          <td class="chig">83.16</td>
                          <td class="chig">83.74</td>
                      </tr>
                      <tr>
                          <td>RAdam</td>
                          <td>61.69</td>
                          <td>74.64</td>
                          <td>75.19</td>
                          <td>76.40</td>
                          <td>75.94</td>
                          <td>77.08</td>
                          <td>74.83</td>
                          <td>72.41</td>
                          <td>72.11</td>
                          <td>79.84</td>
                          <td>82.18</td>
                          <td>82.69</td>
                          <td>84.95</td>
                          <td>84.26</td>
                          <td>86.49</td>
                          <td>78.46</td>
                          <td>79.71</td>
                          <td>84.93</td>
                          <td>81.44</td>
                          <td>82.35</td>
                      </tr>
                      <tr>
                          <td>AdamP</td>
                          <td class="clow">60.27</td>
                          <td>75.56</td>
                          <td>78.17</td>
                          <td class="chig">78.89</td>
                          <td class="chig">77.79</td>
                          <td>78.65</td>
                          <td class="chhg">77.67</td>
                          <td>71.55</td>
                          <td>73.66</td>
                          <td>80.91</td>
                          <td class="chig">84.47</td>
                          <td class="chig">84.40</td>
                          <td class="chig">86.45</td>
                          <td class="chig">86.19</td>
                          <td class="chig">87.16</td>
                          <td class="chig">79.20</td>
                          <td class="chig">81.70</td>
                          <td class="chig">85.15</td>
                          <td>82.12</td>
                          <td class="chig">83.40</td>
                      </tr>
                      <tr>
                          <td>Adan</td>
                          <td>63.98</td>
                          <td>74.90</td>
                          <td>77.08</td>
                          <td class="chhg">79.33</td>
                          <td class="chig">77.73</td>
                          <td>78.43</td>
                          <td class="chig">76.99</td>
                          <td class="chhg">76.33</td>
                          <td class="chig">74.94</td>
                          <td class="chig">83.35</td>
                          <td class="chhg">84.65</td>
                          <td class="chig">84.77</td>
                          <td class="chhg">86.46</td>
                          <td class="chhg">86.75</td>
                          <td class="chhg">87.47</td>
                          <td class="chhg">80.59</td>
                          <td class="chhg">83.23</td>
                          <td class="chhg">85.58</td>
                          <td class="chhg">83.51</td>
                          <td class="chhg">84.89</td>
                      </tr>
                      <tr>
                        <td>AdaBound</td>
                        <td>66.59</td>
                        <td class="chig">77.00</td>
                        <td>78.11</td>
                        <td>75.26</td>
                        <td class="chhg">78.76</td>
                        <td class="chhg">79.88</td>
                        <td>74.14</td>
                        <td class="clow">68.59</td>
                        <td class="clow">70.31</td>
                        <td>80.67</td>
                        <td class="clow">71.96</td>
                        <td>83.90</td>
                        <td class="clow">78.48</td>
                        <td>83.03</td>
                        <td>86.07</td>
                        <td>77.99</td>
                        <td>77.81</td>
                        <td>82.73</td>
                        <td class="chig">83.08</td>
                        <td>82.38</td>
                    </tr>
                    <tr>
                        <td>LARS</td>
                        <td>64.35</td>
                        <td>75.71</td>
                        <td>78.25</td>
                        <td>77.25</td>
                        <td>76.23</td>
                        <td class="cllw">72.43</td>
                        <td>75.50</td>
                        <td>71.36</td>
                        <td>72.64</td>
                        <td class="clow">81.29</td>
                        <td class="cllw">61.40</td>
                        <td>82.22</td>
                        <td class="cllw">33.26</td>
                        <td class="cllw">41.03</td>
                        <td>85.16</td>
                        <td>77.66</td>
                        <td>78.78</td>
                        <td>82.98</td>
                        <td>81.00</td>
                        <td>82.05</td>
                    </tr>
                    <tr>
                        <td>AdaFactor</td>
                        <td>63.91</td>
                        <td>74.49</td>
                        <td>75.41</td>
                        <td>77.03</td>
                        <td>75.38</td>
                        <td>77.83</td>
                        <td>75.03</td>
                        <td class="chig">74.02</td>
                        <td>71.16</td>
                        <td>80.36</td>
                        <td>82.82</td>
                        <td>83.06</td>
                        <td>85.17</td>
                        <td class="chig">85.99</td>
                        <td>86.57</td>
                        <td>78.78</td>
                        <td>78.81</td>
                        <td>84.90</td>
                        <td>81.94</td>
                        <td>82.36</td>
                    </tr>
                      <tr>
                          <td>AdaBelief</td>
                          <td>62.98</td>
                          <td>75.09</td>
                          <td class="chhg">80.53</td>
                          <td class="chig">79.26</td>
                          <td>75.78</td>
                          <td>78.48</td>
                          <td class="chig">76.90</td>
                          <td>70.66</td>
                          <td>73.30</td>
                          <td>80.98</td>
                          <td>83.31</td>
                          <td class="chig">84.47</td>
                          <td>84.80</td>
                          <td>84.54</td>
                          <td>86.64</td>
                          <td>78.55</td>
                          <td class="chig">81.01</td>
                          <td>85.03</td>
                          <td class="chig">83.21</td>
                          <td class="chig">83.56</td>
                      </tr>
                      <tr>
                          <td>NovoGrad</td>
                          <td>64.24</td>
                          <td class="chig">76.09</td>
                          <td class="chig">79.36</td>
                          <td>77.25</td>
                          <td class="clow">71.26</td>
                          <td class="clow">74.23</td>
                          <td>75.16</td>
                          <td>73.13</td>
                          <td class="cllw">67.03</td>
                          <td class="chig">81.82</td>
                          <td>79.99</td>
                          <td>82.01</td>
                          <td class="clow">82.96</td>
                          <td class="clow">80.77</td>
                          <td>85.85</td>
                          <td>77.16</td>
                          <td>78.92</td>
                          <td>83.51</td>
                          <td>81.28</td>
                          <td>82.98</td>
                      </tr>
                      <tr>
                          <td>Sophia</td>
                          <td>64.30</td>
                          <td>74.18</td>
                          <td>75.19</td>
                          <td>77.91</td>
                          <td>76.60</td>
                          <td class="chig">78.95</td>
                          <td>75.85</td>
                          <td>71.47</td>
                          <td>72.74</td>
                          <td>80.61</td>
                          <td class="chig">83.76</td>
                          <td class="chig">83.94</td>
                          <td>85.39</td>
                          <td>84.20</td>
                          <td>86.60</td>
                          <td>77.67</td>
                          <td>78.90</td>
                          <td>84.58</td>
                          <td>81.67</td>
                          <td>82.96</td>
                      </tr>
                      <tr>
                          <td>AdaGrad</td>
                          <td class="cllw">45.79</td>
                          <td class="cllw">71.29</td>
                          <td class="cllw">73.30</td>
                          <td class="clow">51.70</td>
                          <td class="cllw">33.87</td>
                          <td>77.93</td>
                          <td class="cllw">46.06</td>
                          <td class="clow">67.24</td>
                          <td class="clow">67.50</td>
                          <td class="cllw">75.83</td>
                          <td class="clow">75.63</td>
                          <td class="cllw">50.34</td>
                          <td>83.03</td>
                          <td>82.57</td>
                          <td class="clow">66.83</td>
                          <td class="cllw">44.34</td>
                          <td class="cllw">44.40</td>
                          <td class="cllw">79.67</td>
                          <td class="clow">78.71</td>
                          <td class="cllw">38.09</td>
                      </tr>
                      <tr>
                          <td>AdaDelta</td>
                          <td class="chig">66.87</td>
                          <td>74.14</td>
                          <td>75.07</td>
                          <td>76.82</td>
                          <td>75.32</td>
                          <td>77.88</td>
                          <td>74.58</td>
                          <td class="clow">65.44</td>
                          <td>71.32</td>
                          <td>80.25</td>
                          <td class="clow">74.25</td>
                          <td>82.74</td>
                          <td class="clow">81.06</td>
                          <td>84.17</td>
                          <td>85.31</td>
                          <td>85.31</td>
                          <td class="clow">75.91</td>
                          <td class="clow">76.40</td>
                          <td>84.05</td>
                          <td>82.62</td>
                          <td>82.08</td>
                      </tr>
                      <tr>
                          <td>RMSProp</td>
                          <td class="clow">59.33</td>
                          <td>73.30</td>
                          <td>74.25</td>
                          <td>75.45</td>
                          <td>73.94</td>
                          <td>76.83</td>
                          <td>70.71</td>
                          <td>71.63</td>
                          <td class="clow">77.52</td>
                          <td>82.29</td>
                          <td>82.11</td>
                          <td>85.17</td>
                          <td class="clow">61.14</td>
                          <td>86.21</td>
                          <td>77.40</td>
                          <td>77.14</td>
                          <td>84.01</td>
                          <td class="clow">79.72</td>
                          <td>81.83</td>
                      </tr>
                      </tbody>
                  </table>
              </div>
          </div>
      </div>
  </div>
</div>
</section>

<section class="section">
  <div class="container">
      <div class="columns is-centered">
          <div class="column is-full has-text-centered content">
              <h2 class="title is-3">Transfer Learning to Object Detection and 2D Pose Estimation</h2>
              <div class="content">
                  <p class="mt-3">Transfer learning to object detection (Det.) with RetinaNet and 2D pose estimation (Pose.) with TopDown on COCO, evaluated by mAP (%) and AP$^{50}$ (%). We employ pre-trained VGG, ResNet-50 (R-50), Swin-T, and ConvNeXt-T (CX-T) with different pre-training settings, where 100-epoch pre-train by SGD, LARS, or RSB A3 (LAMB), 300-epoch pre-train by AdamW, Adan or RSB A2 (LAMB), and 600-epoch pre-train with RSB A1 (LAMB).</p>
                  <table>
                      <thead>
                          <tr>
                              <th>Pre-training</th>
                              <th colspan="3">2D Pose Estimation</th>
                              <th colspan="8">Object Detection</th>
                          </tr>
                          <tr>
                              <th></th>
                              <th>VGG (SGD)</th>
                              <th>R-50 (SGD)</th>
                              <th>Swin-T (AdamW)</th>
                              <th>VGG (SGD)</th>
                              <th>R-50 (SGD)</th>
                              <th>R-50 (A3)</th>
                              <th>R-50 (A3)</th>
                              <th>R-50 (A2)</th>
                              <th>R-50 (A1)</th>
                              <th>R-50 (Adan)</th>
                              <th>Swin-T (AdamW)</th>
                              <th>CX-T (AdamW)</th>
                          </tr>
                      </thead>
                      <tbody>
                          <tr>
                              <td>SGD-M</td>
                              <td class="clow">47.5</td>
                              <td>71.6</td>
                              <td class="clow">38.4</td>
                              <td>36.6</td>
                              <td class="clow">27.5</td>
                              <td class="clow">28.7</td>
                              <td class="cllw">23.7</td>
                              <td class="clow">34.6</td>
                              <td class="clow">27.5</td>
                              <td class="clow">37.2</td>
                              <td class="clow">38.5</td>
                          </tr>
                          <tr>
                              <td>SGDP</td>
                              <td class="clow">47.3</td>
                              <td class="cllw">41.2</td>
                              <td class="clow">38.9</td>
                              <td>36.6</td>
                              <td class="cllw">17.6</td>
                              <td class="cllw">18.5</td>
                              <td class="clow">26.8</td>
                              <td class="cllw">26.7</td>
                              <td class="cllw">27.4</td>
                              <td class="clow">37.2</td>
                              <td class="cllw">22.5</td>
                          </tr>
                          <tr>
                              <td>LION</td>
                              <td>69.5</td>
                              <td>71.5</td>
                              <td>71.3</td>
                              <td class="clow">32.1</td>
                              <td>35.8</td>
                              <td>35.4</td>
                              <td>37.6</td>
                              <td class="clow">34.6</td>
                              <td class="chig">38.8</td>
                              <td class="chig">41.9</td>
                              <td>42.8</td>
                              <td></td>
                          </tr>
                          <tr>
                              <td>Adam</td>
                              <td class="chig">69.8</td>
                              <td>71.6</td>
                              <td>72.7</td>
                              <td>36.2</td>
                              <td>36.2</td>
                              <td>35.8</td>
                              <td>38.3</td>
                              <td>38.4</td>
                              <td>38.6</td>
                              <td class="chig">41.9</td>
                              <td>43.1</td>
                          </tr>
                          <tr>
                              <td>Adamax</td>
                              <td>69.0</td>
                              <td>71.2</td>
                              <td>72.4</td>
                              <td class="chig">36.8</td>
                              <td>36.8</td>
                              <td>36.4</td>
                              <td>38.3</td>
                              <td>38.4</td>
                              <td>38.3</td>
                              <td>41.5</td>
                              <td>42.0</td>
                          </tr>
                          <tr>
                            <td>NAdam</td>
                            <td>69.7</td>
                            <td class="chig">71.8</td>
                            <td>71.9</td>
                            <td>36.0</td>
                            <td>36.6</td>
                            <td>36.1</td>
                            <td>38.2</td>
                            <td>38.4</td>
                            <td>38.7</td>
                            <td class="chig">41.9</td>
                            <td class="chhg">43.4</td>
                        </tr>
                        <tr>
                            <td>AdamW</td>
                            <td class="chig">70.0</td>
                            <td class="chig">72.0</td>
                            <td class="chhg">72.8</td>
                            <td class="chig">37.1</td>
                            <td class="chig">37.1</td>
                            <td class="chig">36.7</td>
                            <td>38.4</td>
                            <td class="chhg">39.5</td>
                            <td>36.8</td>
                            <td>41.8</td>
                            <td class="chhg">43.4</td>
                        </tr>
                        <tr>
                            <td>LAMB</td>
                            <td>68.5</td>
                            <td>71.5</td>
                            <td>71.7</td>
                            <td class="chig">36.7</td>
                            <td class="chhg">37.5</td>
                            <td class="chhg">37.7</td>
                            <td class="chhg">38.6</td>
                            <td class="chig">38.9</td>
                            <td>38.6</td>
                            <td>41.8</td>
                            <td>42.6</td>
                        </tr>
                        <tr>
                            <td>RAdam</td>
                            <td class="chig">69.8</td>
                            <td class="chig">71.8</td>
                            <td>72.6</td>
                            <td>36.6</td>
                            <td>36.5</td>
                            <td>36.0</td>
                            <td>38.2</td>
                            <td>38.4</td>
                            <td>38.6</td>
                            <td>41.6</td>
                            <td class="chig">43.3</td>
                        </tr>
                          <tr>
                              <td>AdamP</td>
                              <td>69.7</td>
                              <td>71.5</td>
                              <td class="chhg">72.8</td>
                              <td>36.5</td>
                              <td class="chig">37.2</td>
                              <td class="chig">36.5</td>
                              <td class="chig">38.5</td>
                              <td>38.9</td>
                              <td class="chig">38.8</td>
                              <td>41.7</td>
                              <td class="chig">43.3</td>
                          </tr>
                          <tr>
                              <td>Adan</td>
                              <td>69.7</td>
                              <td class="chhg">72.1</td>
                              <td class="chhg">72.8</td>
                              <td class="chhg">37.7</td>
                              <td>37.0</td>
                              <td>36.0</td>
                              <td class="chhg">38.6</td>
                              <td class="chig">39.0</td>
                              <td class="chhg">39.4</td>
                              <td class="chhg">42.0</td>
                              <td>43.2</td>
                          </tr>
                          <tr>
                            <td>AdaBound</td>
                            <td class="cllw">34.0</td>
                            <td class="clow">44.9</td>
                            <td class="cllw">28.4</td>
                            <td>35.9</td>
                            <td>34.2</td>
                            <td class="clow">31.9</td>
                            <td>37.0</td>
                            <td>35.0</td>
                            <td>36.7</td>
                            <td class="clow">38.8</td>
                            <td>41.2</td>
                        </tr>
                        <tr>
                            <td>LARS</td>
                            <td>54.4</td>
                            <td>63.4</td>
                            <td class="clow">47.6</td>
                            <td>35.8</td>
                            <td class="clow">28.9</td>
                            <td class="clow">28.8</td>
                            <td class="clow">34.7</td>
                            <td>36.9</td>
                            <td>37.3</td>
                            <td class="clow">34.6</td>
                            <td>40.5</td>
                        </tr>
                        <tr>
                            <td>AdaFactor</td>
                            <td class="chhg">72.8</td>
                            <td>71.7</td>
                            <td>72.7</td>
                            <td>35.6</td>
                            <td class="chig">37.0</td>
                            <td>36.4</td>
                            <td class="chig">38.5</td>
                            <td>37.8</td>
                            <td>38.7</td>
                            <td>40.5</td>
                            <td>43.1</td>
                        </tr>
                          <tr>
                              <td>AdaBelief</td>
                              <td>69.6</td>
                              <td>67.0</td>
                              <td>61.8</td>
                              <td>36.2</td>
                              <td>34.4</td>
                              <td>33.1</td>
                              <td>36.4</td>
                              <td>38.2</td>
                              <td>38.5</td>
                              <td>40.0</td>
                              <td>41.4</td>
                          </tr>
                          <tr>
                              <td>NovoGrad</td>
                              <td>64.2</td>
                              <td>70.7</td>
                              <td>69.8</td>
                              <td>35.6</td>
                              <td class="clow">27.2</td>
                              <td class="clow">26.3</td>
                              <td>35.2</td>
                              <td class="clow">28.6</td>
                              <td>38.5</td>
                              <td>40.4</td>
                              <td class="clow">39.0</td>
                          </tr>
                          <tr>
                              <td>Sophia</td>
                              <td>69.7</td>
                              <td>71.6</td>
                              <td>72.3</td>
                              <td>36.4</td>
                              <td>35.8</td>
                              <td>35.3</td>
                              <td>38.0</td>
                              <td>38.7</td>
                              <td>37.0</td>
                              <td>40.4</td>
                              <td>42.5</td>
                          </tr>
                          <tr>
                              <td>AdaGrad</td>
                              <td>66.0</td>
                              <td>61.2</td>
                              <td class="clow">48.4</td>
                              <td class="cllw">26.4</td>
                              <td class="clow">21.9</td>
                              <td class="clow">28.3</td>
                              <td class="clow">32.7</td>
                              <td class="clow">27.1</td>
                              <td class="clow">33.7</td>
                              <td class="cllw">32.9</td>
                              <td class="clow">23.7</td>
                          </tr>
                          <tr>
                              <td>AdaDelta</td>
                              <td class="clow">44.3</td>
                              <td class="clow">49.3</td>
                              <td>52.0</td>
                              <td>34.9</td>
                              <td class="clow">32.7</td>
                              <td class="clow">32.7</td>
                              <td>35.9</td>
                              <td class="clow">33.9</td>
                              <td>36.6</td>
                              <td>40.0</td>
                              <td>41.5</td>
                          </tr>
                          <tr>
                              <td>RMSProp</td>
                              <td>68.8</td>
                              <td>71.6</td>
                              <td>72.5</td>
                              <td>35.3</td>
                              <td>36.2</td>
                              <td>35.6</td>
                              <td>37.8</td>
                              <td>38.3</td>
                              <td>38.7</td>
                              <td>41.5</td>
                              <td>43.1</td>
                          </tr>
                      </tbody>
                  </table>
              </div>
          </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container">
      <div class="columns is-centered">
          <div class="column is-full has-text-centered content">
              <h2 class="title is-3">ImageNet-1K Benchmark</h2>
              <div class="content">
                  <p class="mt-3">Top-1 accuracy (%) of DeiT-S and ResNet-50 training 300 epochs by popular optimizers using DeiT and RSB A2 training recipes on ImageNet-1K.</p>
                  <table>
                      <thead>
                          <tr>
                              <th>Backbone</th>
                              <th>DeiT-S (DeiT)</th>
                              <th>R-50 (A2)</th>
                          </tr>
                      </thead>
                      <tbody>
                          <tr>
                              <td>SGD-M</td>
                              <td class="clow">75.35</td>
                              <td>78.82</td>
                          </tr>
                          <tr>
                              <td>SGDP</td>
                              <td>76.34</td>
                              <td>78.02</td>
                          </tr>
                          <tr>
                              <td>LION</td>
                              <td>78.78</td>
                              <td>78.92</td>
                          </tr>
                          <tr>
                              <td>Adam</td>
                              <td>78.44</td>
                              <td>78.16</td>
                          </tr>
                          <tr>
                              <td>Adamax</td>
                              <td>77.71</td>
                              <td>78.05</td>
                          </tr>
                          <tr>
                            <td>NAdam</td>
                            <td>78.26</td>
                            <td>78.97</td>
                          </tr>
                          <tr>
                            <td>AdamW</td>
                            <td class="chig">80.38</td>
                            <td class="chig">79.88</td>
                          </tr>
                          <tr>
                            <td>LAMB</td>
                            <td class="chig">80.23</td>
                            <td class="chig">79.84</td>
                          </tr>
                          <tr>
                            <td>RAdam</td>
                            <td>78.54</td>
                            <td>78.75</td>
                          </tr>
                          <tr>
                              <td>AdamP</td>
                              <td>79.26</td>
                              <td>79.28</td>
                          </tr>
                          <tr>
                              <td>Adan</td>
                              <td class="chhg">80.81</td>
                              <td class="chhg">79.91</td>
                          </tr>
                          <tr>
                            <td>AdaBound</td>
                            <td class="clow">72.96</td>
                            <td class="clow">75.37</td>
                          </tr>
                          <tr>
                            <td>LARS</td>
                            <td class="clow">73.18</td>
                            <td>79.66</td>
                          </tr>
                          <tr>
                            <td>AdaFactor</td>
                            <td>79.98</td>
                            <td>79.36</td>
                          </tr>
                          <tr>
                              <td>AdaBelief</td>
                              <td class="clow">75.32</td>
                              <td>78.25</td>
                          </tr>
                          <tr>
                              <td>NovoGrad</td>
                              <td class="clow">71.26</td>
                              <td class="clow">76.83</td>
                          </tr>
                          <tr>
                              <td>Sophia</td>
                              <td>79.65</td>
                              <td>79.13</td>
                          </tr>
                          <tr>
                              <td>AdaGrad</td>
                              <td class="cllw">54.96</td>
                              <td class="cllw">74.92</td>
                          </tr>
                          <tr>
                              <td>AdaDelta</td>
                              <td class="clow">74.14</td>
                              <td class="clow">77.40</td>
                          </tr>
                          <tr>
                              <td>RMSProp</td>
                              <td>78.03</td>
                              <td>78.04</td>
                          </tr>
                      </tbody>
                  </table>
              </div>
          </div>
      </div>
  </div>
</section>

<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Implementation Details (Vision Backbones)</h2>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
        <div class="column is-full-width">
            <div class="content has-text-justified">
              <table>
                <caption>Three categories of typical vision backbones proposed in the past decade.</caption>
                <thead>
                    <tr>
                        <th>Backbone</th>
                        <th>Date</th>
                        <th>Stage-wise design</th>
                        <th>Block-wise design</th>
                        <th>Operator (feature extractor)</th>
                        <th>Residual branch</th>
                        <th>Input size</th>
                        <th>Training setting</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>AlexNet</td>
                        <td>NIPS'2012</td>
                        <td>-</td>
                        <td>Plain</td>
                        <td>Conv</td>
                        <td>-</td>
                        <td>224</td>
                        <td>PyTorch</td>
                    </tr>
                    <tr>
                        <td>VGG-13</td>
                        <td>ICLR'2015</td>
                        <td>-</td>
                        <td>Plain</td>
                        <td>-</td>
                        <td>Conv</td>
                        <td>-</td>
                        <td>224</td>
                        <td>PyTorch</td>
                    </tr>
                    <tr>
                        <td>ResNet</td>
                        <td>CVPR'2016</td>
                        <td>Hierarchical</td>
                        <td>Bottleneck</td>
                        <td>Conv</td>
                        <td>Addition</td>
                        <td>32</td>
                        <td>PyTorch</td>
                    </tr>
                    <tr>
                        <td>DenseNet</td>
                        <td>CVPR'2017</td>
                        <td>Hierarchical</td>
                        <td>Bottleneck</td>
                        <td>Conv</td>
                        <td>Concatenation</td>
                        <td>32</td>
                        <td>PyTorch</td>
                    </tr>
                    <tr>
                        <td>MobileNet.V2</td>
                        <td>CVPR'2018</td>
                        <td>Hierarchical</td>
                        <td>Inv-bottleneck</td>
                        <td>SepConv</td>
                        <td>Addition</td>
                        <td>224</td>
                        <td>PyTorch</td>
                    </tr>
                    <tr>
                        <td>EfficientNet</td>
                        <td>ICML'2019</td>
                        <td>Hierarchical</td>
                        <td>Inv-bottleneck</td>
                        <td>Conv &amp; SE</td>
                        <td>Addition</td>
                        <td>224</td>
                        <td>RSB A2</td>
                    </tr>
                    <tr>
                        <td>RepVGG</td>
                        <td>CVPR'2021</td>
                        <td>Hierarchical</td>
                        <td>Inv-bottleneck</td>
                        <td>Conv</td>
                        <td>Addition</td>
                        <td>224</td>
                        <td>PyTorch</td>
                    </tr>
                    <tr>
                        <td>DeiT-S (ViT)</td>
                        <td>ICML'2021</td>
                        <td>Patchfy &amp; Isotropic</td>
                        <td>Metaformer</td>
                        <td>Attention</td>
                        <td>PreNorm</td>
                        <td>224</td>
                        <td>DeiT</td>
                    </tr>
                    <tr>
                        <td>MLP-Mixer-S</td>
                        <td>NIPS'2021</td>
                        <td>Patchfy &amp; Isotropic</td>
                        <td>Metaformer</td>
                        <td>MLP</td>
                        <td>PreNorm</td>
                        <td>224</td>
                        <td>DeiT</td>
                    </tr>
                    <tr>
                        <td>Swin Transformer</td>
                        <td>ICCV'2021</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>Local Attention</td>
                        <td>PreNorm</td>
                        <td>224</td>
                        <td>ConvNeXt</td>
                    </tr>
                    <tr>
                        <td>ConvNeXt</td>
                        <td>CVPR'2022</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>MetaNeXt</td>
                        <td>DWConv</td>
                        <td>PreNorm &amp; LayerScale</td>
                        <td>32</td>
                        <td>ConvNeXt</td>
                    </tr>
                    <tr>
                        <td>ConvNeXt.V2</td>
                        <td>CVPR'2023</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>MetaNeXt</td>
                        <td>DWConv</td>
                        <td>PreNorm &amp; LayerScale</td>
                        <td>32</td>
                        <td>ConvNeXt</td>
                    </tr>
                    <tr>
                        <td>MogaNet</td>
                        <td>ICLR'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>DWConv &amp; Gating</td>
                        <td>PreNorm &amp; LayerScale</td>
                        <td>32</td>
                        <td>ConvNeXt</td>
                    </tr>
                    <tr>
                        <td>UniRepLKNet</td>
                        <td>CVPR'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>DWConv &amp; SE</td>
                        <td>PreNorm &amp; LayerScale</td>
                        <td>224</td>
                        <td>ConvNeXt</td>
                    </tr>
                    <tr>
                        <td>TransNeXt</td>
                        <td>CVPR'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>Attention &amp; Gating</td>
                        <td>PreNorm &amp; LayerScale</td>
                        <td>224</td>
                        <td>DeiT</td>
                    </tr>
                    <tr>
                        <td>IdentityFormer</td>
                        <td>TPAMI'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>Identity</td>
                        <td>PreNorm &amp; ResScale</td>
                        <td>224</td>
                        <td>RSB A2</td>
                    </tr>
                    <tr>
                        <td>PoolFormerV2</td>
                        <td>TPAMI'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>Pooling</td>
                        <td>PreNorm &amp; ResScale</td>
                        <td>224</td>
                        <td>RSB A2</td>
                    </tr>
                    <tr>
                        <td>ConvFormer</td>
                        <td>TPAMI'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>SepConv</td>
                        <td>PreNorm &amp; ResScale</td>
                        <td>224</td>
                        <td>RSB A2</td>
                    </tr>
                    <tr>
                        <td>AttentionFormer</td>
                        <td>TPAMI'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>Attention</td>
                        <td>PreNorm &amp; ResScale</td>
                        <td>224</td>
                        <td>RSB A2</td>
                    </tr>
                    <tr>
                        <td>CAFormer</td>
                        <td>TPAMI'2024</td>
                        <td>Patchfy &amp; Hierarchical</td>
                        <td>Metaformer</td>
                        <td>SepConv &amp; Attention</td>
                        <td>PreNorm &amp; ResScale</td>
                        <td>224</td>
                        <td>RSB A2</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</div>
</div>
</section>
 
<style>
.category-a {
    filter: opacity(2.1);
}
.category-b {
    filter: opacity(2.6);
}
.category-c {
    filter: opacity(2.8);
}
.category-d {
    filter: opacity(2.5);
}
</style>

<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Implementation Details (Optimizer)</h2>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
        <div class="column is-full-width">
            <div class="content has-text-justified">
              <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                <caption>Four categories of typical optimizers with their components. From top to bottom are <span class="category-a" style="color: rgb(229,166,175); font-weight: bold;">(a) fixed learning rate with momentum gradient</span>, <span class="category-b" style="color: rgb(154,207,241); font-weight: bold;">(b) adaptive learning rate with momentum gradient</span>, <span class="category-c" style="color: rgb(158,218,200); font-weight: bold;">(c) estimated learning rate with momentum gradient</span>, and <span class="category-d" style="color: rgb(189,184,224); font-weight: bold;">(d) adaptive learning rate with current gradient</span></caption>.
                <thead>
                    <tr>
                        <th>Optimizer</th>
                        <th>Date</th>
                        <th>Learning rate</th>
                        <th>Gradient</th>
                        <th>Weight decay</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>SGD-M</td>
                        <td>TSMC'1971</td>
                        <td>Fixed lr</td>
                        <td>Momentum</td>
                        <td>‚úì</td>
                    </tr>
                    <tr>
                        <td>SGDP</td>
                        <td>ICLR'2021</td>
                        <td>Fixed lr</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>LION</td>
                        <td>NIPS'2023</td>
                        <td>Fixed lr</td>
                        <td>Sign Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>Adam</td>
                        <td>ICLR'2015</td>
                        <td>Estimated second moment</td>
                        <td>Momentum</td>
                        <td>‚úì</td>
                    </tr>
                    <tr>
                        <td>Adamax</td>
                        <td>ICLR'2015</td>
                        <td>Estimated second moment</td>
                        <td>Momentum</td>
                        <td>‚úì</td>
                    </tr>
                    <tr>
                        <td>AdamW</td>
                        <td>ICLR'2019</td>
                        <td>Estimated second moment</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>AdamP</td>
                        <td>ICLR'2021</td>
                        <td>Estimated second moment</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>LAMB</td>
                        <td>ICLR'2020</td>
                        <td>Estimated second moment</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>NAdam</td>
                        <td>ICLR'2018</td>
                        <td>Estimated second moment</td>
                        <td>Nesterov Momentum</td>
                        <td>‚úì</td>
                    </tr>
                    <tr>
                        <td>RAdam</td>
                        <td>ICLR'2020</td>
                        <td>Estimated second moment</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>Adan</td>
                        <td>TPAMI'2023</td>
                        <td>Estimated second moment Nesterov</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>AdaBelief</td>
                        <td>NIPS'2019</td>
                        <td>Estimated second moment variance</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>AdaBound</td>
                        <td>ICLR'2019</td>
                        <td>Estimated second moment</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>AdaFactor</td>
                        <td>ICML'2018</td>
                        <td>Estimated second moment (decomposition)</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>LARS</td>
                        <td>ICLR'2018</td>
                        <td>L2-norm of Gradient</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>Novograd</td>
                        <td>arXiv'2020</td>
                        <td>Sum of estimated second momentum</td>
                        <td>Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>Sophia</td>
                        <td>arXiv'2023</td>
                        <td>Parameter-based estimator</td>
                        <td>Sign Momentum</td>
                        <td>Decoupled</td>
                    </tr>
                    <tr>
                        <td>AdaGrad</td>
                        <td>JMLR'2011</td>
                        <td>Second moment</td>
                        <td>Gradient</td>
                        <td>‚úì</td>
                    </tr>
                    <tr>
                        <td>AdaDelta</td>
                        <td>arXiv'2012</td>
                        <td>Estimated second moment param moment</td>
                        <td>Gradient</td>
                        <td>‚úì</td>
                    </tr>
                    <tr>
                        <td>RMSProp</td>
                        <td>arXiv'2012</td>
                        <td>Estimated second moment</td>
                        <td>Gradient</td>
                        <td>‚úì</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</div>
</div>
</section>

<section class="section">
<div class="columns is-centered has-text-centered">
<div class="column is-six-fifths">
<h2 class="title is-3"> Possible impact of BOCB & Insights </h2>
</div>
</div>
<div class="container">
<div class="columns is-centered">
<div class="column is-full-width">
<div class="content has-text-justified">
<p>
The phenomenon of <strong><em>backbone-optimizer coupling bias (BOCB)</em></strong> we observed during the benchmarking arises from the intricate interplay between the design principles of vision backbones and the inherent properties of optimizers. In particular, we notice that traditional CNNs, such as VGG and ResNets, exhibit a marked coupling with SGD optimizers. In contrast, modern meta-architectures like ViTs and ConvNeXt strongly correlate with adaptive learning rate optimizers, particularly AdamW. As aforementioned, we assume that such a coupling bias may stem from the increasing complexity of optimization as backbone architectures evolve. Concretely, recent backbones incorporate complex elements such as stage-wise hierarchical structures, advanced token-mixers, and block-wise heterogeneous structures. These designs shape a more intricate and challenging optimization landscape, necessitating adaptive learning rate strategies and effective momentum handling. Thus, modern backbones exhibit stronger couplings with optimizers that can navigate these complex landscapes. The BOCB phenomenon has several implications for the design and deployment of vision backbones:
<ul>
<li><b>(A) Deployment</b>: Vision backbones with weaker BOCB offer greater flexibility and are more user-friendly, especially for practitioners with limited resources for extensive hyper-parameter tuning. However, modern architectures like ViTs and ConvNeXt, which exhibit strong coupling with adaptive optimizers, require careful optimizer selection and hyper-parameter tuning for optimal performance.</li>
</ul>
<ul>
<li><b>(B) Backbone Design Insights</b>: While weaker coupling offers more user-friendliness, stronger coupling can potentially lead to better performance and generalization. Tailoring the optimization process to certain architectural characteristics of modern backbones, such as stage-wise hierarchical structures and attention mechanisms for token mixing, can more effectively navigate complex optimization landscapes, unlocking superior performance and generalization capabilities.</li>
</ul>
<ul>
<li><b>(C) Design Principles</b>: The observed BOCB phenomenon highlights the need to consider the coupling between backbone designs and optimizer choices. When designing new backbone architectures, it is crucial to account for both the inductive bias (e.g., hierarchical structures and local operations) and some optimizing auxiliary modules introduced by the macro design principles. A balanced approach that harmonizes the backbone design with the appropriate optimizer choice can lead to optimal performance and efficient optimization, enabling the full potential of the proposed architecture to be realized.</li>
</ul>
</div>
</div>
</div>
</div>
</section>

<section class="section">
<div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
    <h2 class="title is-3"> Origins of BOCB: Backbone Macro Design and Token Mixers </h2>
    </div>
</div>
<div class="container">
    <div class="columns is-centered">
    <div class="column is-full-width">
        <div class="content has-text-justified">
        <p>
            To investigate the causes of <strong><em>BOCB</em></strong>, we first consider what matters the most: optimizers or backbones. As shown in Figure 4 and Table 1, four categories of optimizers show different extents of BOCB with vision backbones. 
            <span class="category-a" style="color: rgb(229,166,175); font-weight: bold;">Category (a)</span> shows a broader performance dispersion, necessitating meticulous hyper-parameter tuning to classical CNNs while demonstrating less adaptability to advanced backbones' optimization demands. 
            <span class="category-b" style="color: rgb(154,207,241); font-weight: bold;">Category (b)</span> and <span class="category-c" style="color: rgb(158,218,200); font-weight: bold;">Category (c)</span> exhibit a robust, hyperparameter-insensitive performance peak, adept at navigating the complex optimization landscapes of primary CNNs and modern DNNs. 
            <span class="category-d" style="color: rgb(189,184,224); font-weight: bold;">Category (d)</span> shows the worst performances and heavy BOCB.
            The trajectory of vision backbone macro design has significantly sculpted the optimization landscape, progressing through distinct phases that reflect the intricate relationship between network architectural complexity and training challenges.
        </p>
        <ul>
            <li><b>Early-stage CNNs</b>: These architectures featured a straightforward design of plainly stacked convolutional and pooling layers, culminated by fully connected layers. Such a paradigm was effective but set the stage for further optimization of landscape alterations.</li>
        </ul>
        <ul>
            <li><b>Classical CNNs</b>: The introduction of ResNet marked a pivotal shift towards stage-wise hierarchical designs, significantly enhancing feature extraction and representation learning ability. ResNet-50, in particular, demonstrated a well-balanced approach to BOCB, which exhibited strong compatibility with SGD optimizers and a relatively lower BOCB compared to its contemporaries.</li>
        </ul>
        <ul>
            <li><b>Modern Architectures</b>: The transition to modern backbones introduced simplified block-wise designs (e.g., MetaNeXt) and ConvNeXt variants, ConvNeXtV2, or complex block-wise heterogeneous structures (e.g.,  MogaNet and UniRepLKNet), increasing the optimization challenge and the degree of BOCB due to their complex feature extraction. Representing a pinnacle in evolution, the MetaFormer architecture incorporates both stage-wise and block-wise heterogeneity into its design. This innovative macro design refines the optimization landscape by harmonizing with optimizers, leading to reduced BOCB and enhanced performance.</li>
        </ul>
        <p style="color: gray; font-size: 1.5em; text-align: center;">For details, please refer to the full paper.</p>
        </div>
    </div>
    </div>
</div>
</section>

<section class="section" id="BibTeX">
<div class="container content">
<h2 class="title">BibTeX</h2>
<pre><code>
@article{li2024battle,
title={Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning},
author={Siyuan Li and Juanxi Tian and Zedong Wang and Luyuan Zhang and Zicheng Liu and Weiyang Jin and Yang Liu and Baigui Sun and Stan Z. Li},
year={2024},
}
</code></pre>
</div>
</section>

<section class="section" id="Contribution">
  <div class="container content">
    <h2 class="title">Contribution</h2>
    <p>The main contributors are:</p>
    <div class="maintainers">
      <div class="maintainer">
        <a href="https://github.com/tianshijing">
          <img src="images/JuanxiTian.png" style="width: 100px; height: 100px; border-radius: 50%; object-fit: cover;" alt="Juanxi Tian">
          <span>@Juanxi Tian</span>
        </a>
      </div>
      <div class="maintainer">
        <a href="https://github.com/Lupin1998">
          <img src="images/SiyuanLi.png" style="width: 100px; height: 100px; border-radius: 50%; object-fit: cover;" alt="Siyuan Li">
          <span>@Siyuan Li</span>
        </a>
      </div>
      <div class="maintainer">
        <a href="https://github.com/Jacky1128">
          <img src="images/ZedongWang.png" style="width: 100px; height: 100px; border-radius: 50%; object-fit: cover;" alt="Zedong Wang">
          <span>@Zedong Wang</span>
        </a>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container content">
  <h2 class="title">Acknowledgement</h2>
  <p>We sincerely thank Zhuang Liu for the insightful discussions and valuable suggestions. This research was primarily conducted by PhD candidate Siyuan Li, intern Juanxi Tian, and Zedong Wang from Westlake University.
  <p class="centered-text">¬© 2024 BOCB. All rights reserved.</p>
  </div>
  </section>


</body>

</html>
